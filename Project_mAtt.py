# -*- coding: utf-8 -*-
"""Copy of Project822.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u0Jo9tjeLeq_ULWLL-9A6_EifZG_Tr8y

Jayani Edirisooriyage - UIN 01241237

CS822- fall 2023

Project

Implementation of manifold attention network for EEG decoding
Followed the steps in "MAtt: A Manifold Attention Network for EEG Decoding"
by Yue-Ting Pan, Jing-Lun Chou, Chun-Shu Wei

Instructions in readme file
"""

import torch
from torch.optim import Optimizer, Adam
from torch.autograd import Function
import torch.nn as nn
from torch.nn.init import orthogonal_
import numpy as np
from scipy import io
import torch.optim as optim
from sklearn.metrics import roc_auc_score as ras
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from scipy.io import loadmat
import os
import torch.nn.functional as F
import seaborn as sns
from scipy import signal

#load train and test data and split for train, validation and test

def load_subject_data(subject_number, data_path):
    # File paths for training and test data
    file_train = f'BCIC_S{subject_number:02d}_T.mat'  # training data file
    file_test = f'BCIC_S{subject_number:02d}_E.mat'   # test data file

    # Load data using scipy's loadmat and the complete file path
    data_train = loadmat(data_path + '/' + file_train)
    data_test = loadmat(data_path + '/' + file_test)

    # Accessing data and labels
    x_train = data_train['x_train']
    y_train = data_train['y_train']
    x_test = data_test['x_test']
    y_test = data_test['y_test']


    # Downsampling from 256 Hz to 128 Hz
    x_train_downsampled = signal.decimate(x_train, 2, axis=-1)
    x_test_downsampled = signal.decimate(x_test, 2, axis=-1)

    # Band-pass filtering at 4-38 Hz
    fs = 128  # Sampling frequency after downsampling
    lowcut = 4
    highcut = 38
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    order = 4  # Filter order
    b, a = signal.butter(order, [low, high], btype='band')

    x_train = signal.filtfilt(b, a, x_train_downsampled, axis=-1)
    x_test = signal.filtfilt(b, a, x_test_downsampled, axis=-1)
    return x_train, y_train, x_test, y_test


def preprocess_bci_subject(x_train,y_train,x_test,y_test,ratio):#split ratio = 8
    # Sort samples based on labels
    s = y_train.argsort()
    x = x_train[s]
    y = y_train[s]

    # stratified split for training and validation sets
    x_train, x_valid, y_train, y_valid = train_test_split(
        x_train, y_train, test_size=ratio, stratify=y_train)#, random_state=42

    # Move tensors to torch
    x_train = torch.Tensor(x_train.copy()).unsqueeze(1)
    y_train = torch.Tensor(y_train.copy()).long()
    x_valid = torch.Tensor(x_valid.copy()).unsqueeze(1)
    y_valid = torch.Tensor(y_valid.copy()).long()
    x_test = torch.Tensor(x_test.copy()).unsqueeze(1)
    y_test = torch.Tensor(y_test.copy()).long()

    # Segmenting at 0.5-4s (438 timepoints)
    x_train = x_train[:, :, :, 124:562]
    x_valid = x_valid[:, :, :, 124:562]
    x_test = x_test[:, :, :, 124:562]

    return x_train, y_train, x_valid, y_valid, x_test, y_test


data_path = '/content/drive/MyDrive/Colab Notebooks/data/BCICIV_2a_mat'
subject_number = 5
best_model_path='/content/drive/MyDrive/Colab Notebooks/data/best_model_bci'

# Load data for a specific subject
x_train, y_train, x_test, y_test = load_subject_data(subject_number, data_path)

#data split for training,  validation and testing
x_train, y_train, x_valid, y_valid, x_test, y_test=preprocess_bci_subject(x_train,y_train,x_test,y_test,8)

def is_nan_or_inf(A):
    C1 = torch.nonzero(torch.isinf(A))
    C2 = torch.nonzero(torch.isnan(A))
    A[torch.isinf(A)] = 0  # Set inf values to zero
    A[torch.isnan(A)] = 0  # Set nan values to zero
    return A
x_train=is_nan_or_inf(x_train)
x_valid=is_nan_or_inf(x_valid)
x_test=is_nan_or_inf(x_test)

y_train=y_train.flatten()
y_valid=y_valid.flatten()
y_test=y_test.flatten()

#autograd functions to use in mattCustomLayers

def symmetric(mat):
    return 0.5*(mat+mat.T)


def LOG(mat):
    batch_size, _, rows, cols = mat.size()
    reshaped_mat = mat.view(-1, rows, cols)  # Reshape to perform SVD on each slice

    U_list, S_list, V_list = [], [], []

    # Perform SVD and log operation on each slice
    for i in range(reshaped_mat.size(0)):
        slice_matrix = reshaped_mat[i]

        # Handle non-finite values by replacing them with a small value
        mask_nan = torch.isnan(slice_matrix)
        mask_inf = torch.isinf(slice_matrix)
        slice_matrix[mask_nan | mask_inf] = 0

        # Perform SVD
        U, S, V = torch.svd(slice_matrix)

        # Replace non-finite values in the singular values matrix with a small value
        eps = 1e-5  # Small value to replace non-finite values
        #S[mask_nan | mask_inf] = eps
        S = torch.where(torch.isnan(S) | torch.isinf(S), torch.tensor(1e-5), S)

        U_list.append(U.unsqueeze(0))
        S_list.append(S.unsqueeze(0).log())
        V_list.append(V.unsqueeze(0))

    # Concatenate results for all slices
    U = torch.cat(U_list, dim=0)
    S = torch.cat(S_list, dim=0)
    V = torch.cat(V_list, dim=0)

    # Reconstruct the output using modified singular values
    Reconstructed_output = torch.bmm(torch.bmm(U, torch.diag_embed(S)), V.transpose(1, 2))
    return Reconstructed_output.view(batch_size, -1, rows, cols)
    pass


def EXPN(mat):
    batch_size, _, rows, cols = mat.size()
    reshaped_mat = mat.view(-1, rows, cols)  # Reshape to perform SVD on each slice

    U_list, S_list, V_list = [], [], []

    # Perform SVD and exponential operation on each slice
    for i in range(reshaped_mat.size(0)):
        slice_matrix = reshaped_mat[i]

        # Handle non-finite values by replacing them with a small value
        mask_nan = torch.isnan(slice_matrix)
        mask_inf = torch.isinf(slice_matrix)
        slice_matrix[mask_nan | mask_inf] = 0

        # Perform SVD
        U, S, V = torch.svd(slice_matrix)

        # Replace non-finite values in the singular values matrix with a small value
        eps = 1e-5  # Small value to replace non-finite values
        #S[mask_nan | mask_inf] = eps
        S = torch.where(torch.isnan(S) | torch.isinf(S), torch.tensor(1e-5), S)

        U_list.append(U.unsqueeze(0))
        S_list.append(S.unsqueeze(0).exp())
        V_list.append(V.unsqueeze(0))

    # Concatenate results for all slices
    U = torch.cat(U_list, dim=0)
    S = torch.cat(S_list, dim=0)
    V = torch.cat(V_list, dim=0)

    # Reconstruct the output using modified singular values
    Reconstructed_output = torch.bmm(torch.bmm(U, torch.diag_embed(S)), V.transpose(1, 2))
    return Reconstructed_output.view(batch_size, -1, rows, cols)
    pass

class SPD_TangentSpace(Function):
    @staticmethod
    def forward(ctx, input):
        output = torch.empty_like(input)
        for epoch, x in enumerate(input):
            U, S, V = torch.svd(x)
            S_log = torch.log(S)
            tangent_space = U.mm(torch.diag(S_log).mm(U.transpose(0, 1)))
            output[epoch] = tangent_space
        ctx.save_for_backward(input)
        return output

    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        grad_input = torch.empty_like(input)
        if input.requires_grad:
            identity = torch.eye(input.size(1)).to(input)
            grad_input = torch.empty_like(input)
            for epoch, grad in enumerate(grad_output):
                x = input[epoch]
                U, S, V = torch.svd(x)
                grad = symmetric(grad)
                s_log_diagonal = torch.log(S).diag()
                s_inv_diagonal = (1 / S).diag()
                dLossdV = 2 * (grad.mm(U.mm(s_log_diagonal)))
                dLossdS = identity * (s_inv_diagonal.mm(U.transpose(0, 1).mm(grad.mm(U))))
                S = S.unsqueeze(1)
                n = S.size(0)
                P = S.expand(n, n)
                P = P - P.t()
                mask_zero = torch.abs(P) == 0
                P[mask_zero] = 0
                P = 1 / P
                grad_input[epoch] = U.mm(symmetric(P.t() * (U.transpose(0, 1).mm(dLossdV))) + dLossdS).mm(U.transpose(0, 1))
        return grad_input

# function to make eig vals greater than epsilon=1e-4 - strictly SPD
class SPDRectified(Function):
    @staticmethod
    def forward(ctx, input, epsilon=1e-5):
        ctx.epsilon = epsilon
        ctx.save_for_backward(input)
        output = input.clone()
        with torch.no_grad():
            for i in range(input.size(0)):
                U, S, V = torch.svd(input[i])
                S[S < ctx.epsilon] = ctx.epsilon
                output[i] = U.mm(torch.diag(S).mm(U.t()))
        return output

    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        grad_input = torch.zeros_like(input)

        if ctx.needs_input_grad[0]:
            epsilon = ctx.epsilon
            identity = torch.eye(input.size(1), device=input.device)

            for i in range(input.size(0)):
                U, S, V = torch.svd(input[i])
                grad = symmetric(grad_output[i])

                S_mask = S > epsilon
                S_thresholds_diag = S.clone()
                S_thresholds_diag[~S_mask] = epsilon
                S_thresholds_diag = S_thresholds_diag.diag()
                Ss = S_mask.float().diag()

                dLossdV = 2 * (grad.mm(U.mm(S_thresholds_diag)))
                dLossdS = identity * (Ss.mm(U.t().mm(grad.mm(U))))

                S_expanded = S.unsqueeze(1).expand_as(input[i])
                P = 1 / (S_expanded - S_expanded.t()).masked_fill(torch.abs(S_expanded - S_expanded.t()) == 0, 0)

                grad_input[i] = U.mm(symmetric(P.t() * (U.t().mm(dLossdV))) + dLossdS).mm(U.t())

        return grad_input, None

class mAttCustomLayers(nn.Module):
    def __init__(self,epochs, input_mat_size, output_mat_size):
        super(mAttCustomLayers, self).__init__()
        # Stielfel parameters as trainable parameters
        self.Weight_Q = nn.Parameter(torch.FloatTensor(input_mat_size, output_mat_size))
        orthogonal_(self.Weight_Q)

        self.Weight_K = nn.Parameter(torch.FloatTensor(input_mat_size, output_mat_size))
        orthogonal_(self.Weight_K)

        self.Weight_V = nn.Parameter(torch.FloatTensor(input_mat_size, output_mat_size))
        orthogonal_(self.Weight_V)

        self.conLayer1 = nn.Conv2d(1, 22, (22, 1))  # 22 channels in input
        self.BnormL1 = nn.BatchNorm2d(22)
        self.conLayer2 = nn.Conv2d(22, 20, (1, 12), padding=(0, 6))
        self.BnormL2 = nn.BatchNorm2d(20)
        self.FC=nn.Linear(18*18,4,bias=True)
    #def forward(self, input,epochs):
    def forward(self, input_data,epochs):
        #conLayer1=nn.Conv2d(1,22,(22,1))# 22 channels in input
        X=self.conLayer1(input_data)
        #BnormL1=nn.BatchNorm2d(22)
        X=self.BnormL1(X)
        #conLayer2=nn.Conv2d(22,20,(1,12),padding=(0,6))
        X=self.conLayer2(X)
        #BnormL2=nn.BatchNorm2d(20)
        X=self.BnormL2(X)

        def signal2SPD(x):
                x = x.squeeze()
                mean = x.mean(dim=-1, keepdim=True).expand_as(x)
                x = x - mean
                x2=x.permute(0, 2, 1)
                COVx = torch.bmm(x,x2)
                n = x.shape[-1] - 1
                COVx = COVx / n
                COV_TRACE = COVx.diagonal(offset=0, dim1=-1, dim2=-2).sum(-1)
                COV_TRACE = COV_TRACE.unsqueeze(1).unsqueeze(2)
                COVx /= COV_TRACE
                #IDENTITY = torch.eye(x.shape[-1], dtype=COVx.dtype).repeat(x.shape[0], 1, 1)
                #COVx = COVx + (1e-5 * IDENTITY)
                IDENTITY = torch.eye(COVx.shape[-1], dtype=COVx.dtype).unsqueeze(0).expand_as(COVx)
                COVx = COVx + (1e-5 * IDENTITY)#1e-5
                return COVx
        def segment_length(n, epochs):
                epochs = int(epochs)
                base_size = n // epochs
                lengths = []
                for i in range(epochs):
                    lengths.append(base_size)
                for i in range(n - base_size * epochs):
                    lengths[i] = lengths[i] + 1
                if sum(lengths) != n:
                    raise ValueError('epoch size error!')
                else:
                    return lengths

        def E2RFunction(x,epochs):
            xlengths = segment_length(x.shape[-1], epochs)
            x_splits = list(torch.split(x, xlengths, dim=-1))
            x_SPD_list = [signal2SPD(j) for j in x_splits]
            x = torch.stack(x_SPD_list)
            x = x.permute(1, 0, 2, 3)
            return x # returns covariance matrices in SPD manifold


        def SPD_mat_transforms(INPUT,input_mat_size,output_mat_size,Weight_Q,Weight_K,Weight_V):
            SPD_transformed_OUTPUT=INPUT #initialize output
            #initialize weights as stiefel parameters
            #Weight=Stiefel_Parameters(input_mat_size,output_mat_size)
            #expand the weights for the batch/epoch size
            INPUT=INPUT.reshape(-1, INPUT.size(2), INPUT.size(3))
            Weight_Q = Weight_Q.unsqueeze(0)
            Weight_Q = Weight_Q.expand(INPUT.size(0),-1,-1)
            SPD_transformed_OUTPUT_Q = torch.bmm(Weight_Q.transpose(1, 2), torch.bmm(INPUT, Weight_Q))
            Weight_K = Weight_K.unsqueeze(0)
            Weight_K = Weight_K.expand(INPUT.size(0),-1,-1)
            SPD_transformed_OUTPUT_K = torch.bmm(Weight_K.transpose(1, 2), torch.bmm(INPUT, Weight_K))
            Weight_V = Weight_V.unsqueeze(0)
            Weight_V = Weight_V.expand(INPUT.size(0),-1,-1)
            SPD_transformed_OUTPUT_V = torch.bmm(Weight_V.transpose(1, 2), torch.bmm(INPUT, Weight_V))
            stiefel_param=[Weight_Q,Weight_K,Weight_V]
            return SPD_transformed_OUTPUT_Q,SPD_transformed_OUTPUT_K,SPD_transformed_OUTPUT_V#,stiefel_param



        def LOG(mat):
            batch_size, _, rows, cols = mat.size()
            reshaped_mat = mat.view(-1, rows, cols)  # Reshape to perform SVD on each slice

            U_list, S_list, V_list = [], [], []

            # Perform SVD and log operation on each slice
            for i in range(reshaped_mat.size(0)):
                slice_matrix = reshaped_mat[i]

                # Handle non-finite values by replacing them with a small value
                mask_nan = torch.isnan(slice_matrix)
                mask_inf = torch.isinf(slice_matrix)
                slice_matrix[mask_nan | mask_inf] = 0

                # Perform SVD
                U, S, V = torch.svd(slice_matrix)

                # Replace non-finite values in the singular values matrix with a small value
                eps = 1e-5  # Small value to replace non-finite values
                #S[mask_nan | mask_inf] = eps
                S = torch.where(torch.isnan(S) | torch.isinf(S), torch.tensor(1e-5), S)

                U_list.append(U.unsqueeze(0))
                S_list.append(S.unsqueeze(0).log())
                V_list.append(V.unsqueeze(0))

            # Concatenate results for all slices
            U = torch.cat(U_list, dim=0)
            S = torch.cat(S_list, dim=0)
            V = torch.cat(V_list, dim=0)

            # Reconstruct the output using modified singular values
            Reconstructed_output = torch.bmm(torch.bmm(U, torch.diag_embed(S)), V.transpose(1, 2))
            return Reconstructed_output.view(batch_size, -1, rows, cols)
            pass

        def EXPN(mat):
            batch_size, _, rows, cols = mat.size()
            reshaped_mat = mat.view(-1, rows, cols)  # Reshape to perform SVD on each slice

            U_list, S_list, V_list = [], [], []

            # Perform SVD and exponential operation on each slice
            for i in range(reshaped_mat.size(0)):
                slice_matrix = reshaped_mat[i]

                # Handle non-finite values by replacing them with a small value
                mask_nan = torch.isnan(slice_matrix)
                mask_inf = torch.isinf(slice_matrix)
                slice_matrix[mask_nan | mask_inf] = 0

                # Perform SVD
                U, S, V = torch.svd(slice_matrix)

                # Replace non-finite values in the singular values matrix with a small value
                eps = 1e-5  # Small value to replace non-finite values
                #S[mask_nan | mask_inf] = eps
                S = torch.where(torch.isnan(S) | torch.isinf(S), torch.tensor(1e-5), S)

                U_list.append(U.unsqueeze(0))
                S_list.append(S.unsqueeze(0).exp())
                V_list.append(V.unsqueeze(0))

            # Concatenate results for all slices
            U = torch.cat(U_list, dim=0)
            S = torch.cat(S_list, dim=0)
            V = torch.cat(V_list, dim=0)

            # Reconstruct the output using modified singular values
            Reconstructed_output = torch.bmm(torch.bmm(U, torch.diag_embed(S)), V.transpose(1, 2))
            return Reconstructed_output.view(batch_size, -1, rows, cols)
            pass

        def LOG_EUCLIDEAN_Distance(L1, L2):
            logL1=LOG(L1)
            logL2=LOG(L2)
            log_diff=logL1-logL2
            logS_term= log_diff.permute(0, 1, 3, 2)#square of frobinus norm
            log_distances=torch.sum(logS_term,dim=-1)
            return log_distances
            pass

        def LOG_EUCLIDEAN_MEAN(weight, COVmat):
            #weight[epochs,num_mats,num_mats], mat[epochs,num_mat,n,n]
            #transform onto log space
            Log_COVmat = LOG(COVmat)  # Resize
            weight=weight.permute(0, 1, 3, 2)
            Weighted_sum = torch.matmul(weight,Log_COVmat)
            Weighted_sum = Weighted_sum #.view(COVmat.shape)  # Reshape to original shape
            Log_euc_mean = EXPN(Weighted_sum)
            return Log_euc_mean
            pass
        #E2R conversion
        X=signal2SPD(X)
        X=E2RFunction(X,epochs)
        # manifold attention mechanism
        Q_tf,K_tf,V_tf=SPD_mat_transforms(X,20,18,self.Weight_Q,self.Weight_K,self.Weight_V)
        Q = Q_tf.view(X.shape[0], X.shape[1], 18,18)
        K = K_tf.view(X.shape[0], X.shape[1], 18,18)
        V = V_tf.view(X.shape[0], X.shape[1], 18,18)

        # Attention scores
        Qexp = Q.repeat(1, V.shape[1], 1, 1) # repeat Q to match V tensor dim 2
        Kexp = K.unsqueeze(2).repeat(1, 1, V.shape[1], 1, 1) # expand to the same dim2 in V
        Kexp = Kexp.view(Kexp.shape[0], Kexp.shape[1] * Kexp.shape[2], Kexp.shape[3], Kexp.shape[4])
        similarity=LOG_EUCLIDEAN_Distance(Qexp,Kexp).view(V.shape[0],V.shape[1],V.shape[1],V.shape[3])
        Att_sim=LOG(1+similarity)
        Attention_Probability = torch.nn.functional.softmax((1 / (1 + Att_sim)), dim=-3).permute(0, 2, 3, 1)
        scores=LOG_EUCLIDEAN_MEAN(Attention_Probability,V) #.view(V.shape[0],V.shape[1],18,18)
        dims = list(scores.shape[:2])
        dims.append(scores.shape[-1])

        scores=scores.repeat(1, 2, 1, 1)
        #X = scores.reshape(-1,18,18)
        X = scores.reshape(-1,18,18)
        X = SPDRectified.apply(X,1e-5)# epsilon=1e-4
        # Riemannian to Euclidean
        X = SPD_TangentSpace.apply(X)
               #X=torch.flatten(X,start_dim=-1)

        #flatten
        #X = X.view(X.size(0), -1)

        #Fully connected layer
        #FC=nn.Linear(X.size(1),4,bias=True)

        # Reshape input data to match the linear layer's expected input size
        # Interpolate the input to match the linear dimensions
        interpolated_input = F.interpolate(X.unsqueeze(0), size=(18, 18), mode='bilinear', align_corners=False).squeeze(0)
        X = interpolated_input.view(X.shape[0],X.shape[1]*X.shape[2])  # Reshape input for linear layer
        X=self.FC(X)
       # X = X.view(X.size(0),4)
        X=F.softmax(X,dim=0)
        return X    # final prediction of model

#optimizer to handle SPD with existing ADAM optimizer for Loss

def symmetric(mat):
    return 0.5*(mat+mat.T)

def orthogonal_proj(grad,mat):
   out_mat=grad-mat.mm(symmetric(mat.t().mm(grad)))
   return out_mat

#Retraction operation updating the matrix based on ref matrix using QR decomposition to make sure no -ve values in the o/p matrix
def Retraction(mat,ref_mat):
  D=mat+ref_mat
  Q,R=D.qr()  # QR decomposition of the D matrix
  # make sure all diagonal elements are Positive Definite
  pos_D=(R.diag().sign()+0.5).sign().diag()
  out_mat=Q.mm(pos_D)
  return out_mat
######################

class CustomOptimizer(Optimizer):
    def __init__(self, params, lr=5e-4, weight_decay=1e-1):
        defaults = dict(lr=lr, weight_decay=weight_decay)
        super(CustomOptimizer, self).__init__(params, defaults)
        self.adam_optimizer = Adam(params, lr=lr, weight_decay=weight_decay)

    def step(self, closure=None):
        loss = None

        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue

                grad = p.grad.data
                if grad.is_sparse:
                    raise RuntimeError("sparse gradients")
                state = self.state[p]

                if len(state) == 0:
                    state['step'] = 0
                state['step'] += 1

        loss = self.adam_optimizer.step(closure)
        retraction_params = []  # Initialize list for parameters that need retraction

        for group in self.param_groups:
            for p in group['params']:
                if isinstance(p, nn.Parameter) and p in [self.Weight_Q, self.Weight_K, self.Weight_V]:  # Specify parameters here
                    state = self.state[id(p)]
                    trans = Retraction(p.data, state)  # Perform retraction
                    p.data.fill_(0).add_(trans)  # Update parameter using retraction
                    retraction_params.append(p)
        print(retraction_params)  # Print parameters that underwent retraction
        loss.backward()  # Backpropagation
        return loss

def train_and_test_network(x_train, y_train, x_valid, y_valid, x_test, y_test,
                           iterations=400, lr=5e-4,
                           weight_decay=1e-1, repeat=1, sub=1, epochs=3,
                           input_mat_size=20, output_mat_size=18, best_model_path=None):
    # Initialization
    model = mAttCustomLayers(epochs=3, input_mat_size=20, output_mat_size=18)
    #stiefel_params = [model.Weight_Q, model.Weight_K, model.Weight_V]

    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    criterion = nn.CrossEntropyLoss()
    best_loss = float('inf')
    validation_losses = []

    for i in range(iterations):
        model.train()
        tr_len = len(y_train)
        acc_tr = 0

        out = model(x_train, 3)
        loss = criterion(out, y_train.squeeze())

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        _, predicted = torch.max(out, 1)
        acc_tr += (predicted == y_train.squeeze()).sum().item()
        #for i in range(len(y_train)):
        #    print(f"Predicted: {predicted[i].item()}, Actual: {y_train[i].item()}")
        model.eval()
        validation_loss, _ = validate_network(model, x_valid, y_valid, criterion)
        validation_losses.append(validation_loss)

        print(f'Iteration {i + 1} - train_acc: {acc_tr/tr_len:.4f}, val_loss: {validation_loss:.4f}')
       # print(out,predicted)
        if validation_loss < best_loss:
            if best_model_path is not None and not os.path.exists(best_model_path):
                os.makedirs(best_model_path)

            print(f'Iteration {i + 1} - Saving model...')
            best_loss = validation_loss
            final_model_path = os.path.join(best_model_path, f'repeat{repeat}_sub{sub}_epochs{epochs}_lr{lr}_wd{weight_decay}.pt')
            torch.save(model.state_dict(), final_model_path)

    # Plot validation loss
    plt.plot(validation_losses, label='Validation Loss')
    plt.xlabel('Iterations')
    plt.ylabel('Loss')
    plt.title('Validation Loss during Training')
    plt.legend()
    plt.show()


    # Load best performing model
    trained_net = mAttCustomLayers(epochs, input_mat_size, output_mat_size)
    trained_net.load_state_dict(torch.load(final_model_path))
    test_acc,conf_matrix = test_network(trained_net, x_test, y_test)

    # Plot the heatmap
    plt.figure(figsize=(10, 8))
    sns.heatmap(conf_matrix, annot=True, fmt=".2f", cmap="YlGnBu", cbar=False)
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title("Confusion Matrix Heatmap")
    plt.show()


    print(f'Best Model Test Accuracy: {test_acc}')
    return trained_net, test_acc

def validate_network(model, x_valid, y_valid, criterion):
    model.eval()
    val_loss = 0
    validation_acc = 0
    validation_len = len(y_valid)

    with torch.no_grad():
        out2 = model(x_valid, 3)
        val_loss += criterion(out2, y_valid.squeeze()).item()
        _, predicted = torch.max(out2, 1)
        validation_acc += (predicted == y_valid.squeeze()).sum().item()

    return val_loss / validation_len, validation_acc / validation_len

def test_network(model, x_test, y_test):
    model.eval()
    acc = 0
    test_len = len(y_test)

    with torch.no_grad():
        out3 = model(x_test, 3)
        _, predicted = torch.max(out3, 1)
        acc += (predicted == y_test.squeeze()).sum().item()
        # Reshape the predicted tensor to create a confusion matrix
        conf_matrix = torch.zeros(len(torch.unique(y_test)), len(torch.unique(y_test)))
        for t, p in zip(y_test.view(-1), predicted.view(-1)):
          conf_matrix[t.long(), p.long()] += 1

    return acc / test_len, conf_matrix

#training BCI2a



trained_net, test_acc=train_and_test_network(x_train, y_train, x_valid, y_valid, x_test, y_test,iterations=400, lr=5e-4,weight_decay=1e-1, repeat=1, sub=1,epochs=3,input_mat_size=20,output_mat_size=18,best_model_path=best_model_path)
